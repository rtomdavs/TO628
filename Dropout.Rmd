---
title: "College Dropout Prediction"
author: "The Kumar Kids"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: spacelab
    highlight: zenburn
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Getting Data Ready for Analysis

## Reading and Cleaning
```{r}
#Reading in the dataset
drops <- read.csv("dataset.csv")

#Dropping Parents' Qualifications
drops$Mother.s.occupation <- NULL
drops$Mother.s.qualification <- NULL
drops$Father.s.occupation <- NULL
drops$Father.s.qualification <- NULL

#Converting appropriate columns into factor
#factorcols <- c("Marital.status", "Application.mode", "Daytime.evening.attendance", "Course", "Previous.qualification",
#          "Nacionality", "Mother.s.qualification", "Father.s.qualification", "Mother.s.occupation",
#          "Father.s.occupation", "Displaced", "Educational.special.needs", "Debtor", "Tuition.fees.up.to.date",
#          "Gender", "Scholarship.holder", "International")

factorcols <- c("Marital.status", "Application.mode", "Daytime.evening.attendance", "Course", "Previous.qualification",
          "Nacionality", "Displaced", "Educational.special.needs", "Debtor", "Tuition.fees.up.to.date",
          "Gender", "Scholarship.holder", "International")


drops[factorcols] <- lapply(drops[factorcols], factor)
str(drops)

#Removing the enrolled students, for later prediction
nonenr <- subset(drops, drops$Target != "Enrolled")


#Factorizing target
nonenr$Target <- as.factor(nonenr$Target)


#Converting to Model Matrix
dummydrop <- as.data.frame(model.matrix(~., nonenr))

#summary(nonenr)
```

## Creating Normalized Version of Data
```{r}
# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
randomdrop <- dummydrop[sample(nrow(dummydrop)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
drop_norm <- as.data.frame(lapply(randomdrop, normalize))
drop_norm$X.Intercept. <- NULL
drop_norm$Nacionality13 <- NULL
drop_norm$Nacionality20 <- NULL
```

# Initial Model Creation

## Testing and Training Sets
```{r}
#Setting Seed for Ease of Replication
set.seed(12345)
trainrows <- sample(nrow(drop_norm), 0.8*nrow(drop_norm))

#Training and Testing Sets with full columns
droptrain <- drop_norm[trainrows, ]
droptest <- drop_norm[-trainrows, ]

#Sets without Y variable
knn_train <- drop_norm[trainrows, -match("TargetGraduate", names(drop_norm))]
knn_test <- drop_norm[-trainrows, -match("TargetGraduate", names(drop_norm))]

#Sets with just Y variable
train_labels <- drop_norm[trainrows, "TargetGraduate"]
test_labels <- drop_norm[-trainrows, "TargetGraduate"]
```


## Building Logistic Model
```{r}
logmod <- glm(TargetGraduate ~., data = droptrain, family = "binomial")
```

## Testing Logistic Model
```{r}
library(caret)

Logpred <- predict(logmod, droptest)
pblog <- ifelse(Logpred >= 0.5, 1, 0)
confusionMatrix(as.factor(pblog), as.factor(droptest$Target), positive = "1")

#Accuracy of 88.98%
#Kappa of 0.7665
```

## KNN Model Creation
```{r, cache=TRUE}
library(class)

set.seed(12345)
KNNmodel <- knn(knn_train, knn_test, train_labels, k = 60)
```

## KNN Model Testing
```{r, cache=TRUE}
confusionMatrix(as.factor(KNNmodel), as.factor(test_labels), positive = "1")

#Accuracy of 80.85%
#Kappa of 0.5489
```

## ANN Model Creation
```{r, cache= TRUE}
library(neuralnet)
model_ann <- neuralnet(TargetGraduate ~ ., data = droptrain, hidden = 1)
```

## ANN Model Testing
```{r, cache= TRUE}
library (caret)
ann_pred <- predict(model_ann, droptest)
predbin_ann <- ifelse(ann_pred >= 0.5, 1, 0)
confusionMatrix(as.factor(predbin_ann), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 89.39%
#Kappa of 0.7723
```

## Decision Tree Model Creation
```{r, cache=TRUE}
library(C50)

droptree <- C5.0(as.factor(TargetGraduate) ~., data=droptrain)
```

## Decision Tree Model Testing
```{r, cache=TRUE}
tree_pred <- predict(droptree, droptest)
confusionMatrix(as.factor(tree_pred), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 90.36%
#Kappa of 0.7899
```

## Random Forest Model Creation
```{r, cache=TRUE}
library(randomForest)

forestmodel <- randomForest(as.factor(TargetGraduate) ~., data = droptrain)

varImpPlot(forestmodel)
```


## Random Forest Model Testing
```{r, cache=TRUE}
forest_pred <- predict(forestmodel, droptest)

confusionMatrix(as.factor(forest_pred), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 91.18%
#Kappa for this model is 0.8095
```

## Individual Initial Model Results
So far our most accurate model in predicting college dropouts has been our Random Forest model. It has the highest Kappa at 0.8095, and the highest accuracy at 91.18%. Though it isn't the most accurate model, we can also use our decision tree to find out which of our students' characteristics are the biggest predictors of dropping out.

Attribute Usage:

- 100.00%	Curricular.units.2nd.sem..approved.
- 72.73%	Tuition.fees.up.to.date1
- 58.47%	Curricular.units.2nd.sem..credited.
- 56.23%	Debtor1
- 32.99%	Course2


# Combining Models for a Stacked Decision Tree

## Making New Data Frames with Predictions
```{r}
# Adding Prediction Columns for Every Model
dropstack <- data.frame(droptest$TargetGraduate, Logpred, KNNmodel, ann_pred, tree_pred, forest_pred)

# Renaming Columns
colnames(dropstack)[1] = "Graduated"
colnames(dropstack)[3] = "KNNPred"
colnames(dropstack)[4] = "ANNPred"
colnames(dropstack)[5] = "TreePred"
colnames(dropstack)[6] = "ForestPred"

summary(dropstack)
```

## Test and Train for Stacked Data
```{r}
set.seed(12345)
trainrows <- sample(nrow(dropstack), 0.8*nrow(dropstack))

stack_train <- dropstack[trainrows, ]
stack_test <- dropstack[-trainrows, ]
```

## Decision Tree on Telestack
```{r}
stack_tree <- C5.0(as.factor(Graduated) ~., data = stack_train)

plot(stack_tree)
```

## Evaluating Decision Tree on Telestack
```{r}
stack_tree_pred <- predict(stack_tree, stack_test)

confusionMatrix(as.factor(stack_tree_pred), as.factor(stack_test$Graduated), positive = "1")
```


