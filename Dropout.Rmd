---
title: "College Dropout Prediction"
author: "The Kumar Kids"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: spacelab
    highlight: zenburn
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

### Background of the Data

In this report, we will be analyzing a dataset of students who have either graduated, dropped out, or are still enrolled from a four year undergraduate degree. This data, which is amalgamated from several different sources, includes information on demographic, socioeconomic, macroeconomic, and academic factors for these students. The success of students in higher education is vital to the overall productivity of an economy, as human capital is a huge driver of growth.

### Structure of the Data

This dataset has 31 overall columns, with 1 being our target variable and the others being characteristics of both the students and their environment. We have demographic information like marital status, gender, and age of the students. We also have financial information of the students like whether they are a debtor, have their tuition fees up to date, or are on a scholarship. There are also academic characteristics of these students such as their approved credits, course of study, or whether they have educational special needs. Lastly, there are macroeconomic indicators as well as they are strongly correlated with trends in education.

Our target variable for training the model will be whether or not the students graduated, and we will then use the models we built on the students that are currently enrolled to predict whether they have a high likelihood of dropping out.

### Use Case

We will be posing as a consulting group hired on by the board of directors of the Polytechnic Institute Of Portalegre. We were brought on in order to figure out why they have a much higher dropout rate than the average in the United States. Reports indicate that around 33% of undergraduates do not complete their degree program. Whereas in our data it appears that, among the combined students that have either dropped out or graduated, around 40% of the students did not graduate. Knowing the characteristics of students who are likely to drop out, the academic advisors could further evaluate the situation of these “likely-to-dropout” students case by case and reach out to offer academic planning/support to reduce their likelihood of dropping out. With the number of enrolled students who would likely drop out, the finance office can estimate the amount of tuition the university would lose per year with this number. The university admission office could also use this number to decide the number of transfer students we can allow next year to fill the gap. 



# Getting Data Ready for Analysis

## Reading and Cleaning
```{r}
#Reading in the dataset
drops <- read.csv("dataset.csv")

#Dropping Parents' Qualifications
drops$Mother.s.occupation <- NULL
drops$Mother.s.qualification <- NULL
drops$Father.s.occupation <- NULL
drops$Father.s.qualification <- NULL

#Converting appropriate columns into factor
#factorcols <- c("Marital.status", "Application.mode", "Daytime.evening.attendance", "Course", "Previous.qualification",
#          "Nacionality", "Mother.s.qualification", "Father.s.qualification", "Mother.s.occupation",
#          "Father.s.occupation", "Displaced", "Educational.special.needs", "Debtor", "Tuition.fees.up.to.date",
#          "Gender", "Scholarship.holder", "International")

factorcols <- c("Marital.status", "Application.mode", "Daytime.evening.attendance", "Course", "Previous.qualification",
          "Nacionality", "Displaced", "Educational.special.needs", "Debtor", "Tuition.fees.up.to.date",
          "Gender", "Scholarship.holder", "International")


drops[factorcols] <- lapply(drops[factorcols], factor)


#Removing the enrolled students, for later prediction
nonenr <- subset(drops, drops$Target != "Enrolled")


#Factorizing target
nonenr$Target <- as.factor(nonenr$Target)


#Converting to Model Matrix
dummydrop <- as.data.frame(model.matrix(~., nonenr))

#summary(nonenr)
```

## Creating Normalized Version of Data
```{r}
# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
randomdrop <- dummydrop[sample(nrow(dummydrop)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
drop_norm <- as.data.frame(lapply(randomdrop, normalize))
drop_norm$X.Intercept. <- NULL
drop_norm$Nacionality13 <- NULL
drop_norm$Nacionality20 <- NULL
```




# Initial Model Creation

## Testing and Training Sets
```{r}
#Setting Seed for Ease of Replication
set.seed(12345)
trainrows <- sample(nrow(drop_norm), 0.8*nrow(drop_norm))

#Training and Testing Sets with full columns
droptrain <- drop_norm[trainrows, ]
droptest <- drop_norm[-trainrows, ]

#Sets without Y variable
knn_train <- drop_norm[trainrows, -match("TargetGraduate", names(drop_norm))]
knn_test <- drop_norm[-trainrows, -match("TargetGraduate", names(drop_norm))]

#Sets with just Y variable
train_labels <- drop_norm[trainrows, "TargetGraduate"]
test_labels <- drop_norm[-trainrows, "TargetGraduate"]
```


## Building Logistic Model
```{r}
logmod <- glm(TargetGraduate ~., data = droptrain, family = "binomial")
```

## Testing Logistic Model
```{r}
library(caret)

Logpred <- predict(logmod, droptest)
pblog <- ifelse(Logpred >= 0.5, 1, 0)
confusionMatrix(as.factor(pblog), as.factor(droptest$Target), positive = "1")

#Accuracy of 88.98%
#Kappa of 0.7665
```

## KNN Model Creation
```{r, cache=TRUE}
library(class)

set.seed(12345)
KNNmodel <- knn(knn_train, knn_test, train_labels, k = 60)
```

## KNN Model Testing
```{r, cache=TRUE}
confusionMatrix(as.factor(KNNmodel), as.factor(test_labels), positive = "1")

#Accuracy of 80.85%
#Kappa of 0.5489
```

## ANN Model Creation
```{r, cache= TRUE}
library(neuralnet)
model_ann <- neuralnet(TargetGraduate ~ ., data = droptrain, hidden = 1)
```

## ANN Model Testing
```{r, cache= TRUE}
library (caret)
ann_pred <- predict(model_ann, droptest)
predbin_ann <- ifelse(ann_pred >= 0.5, 1, 0)
confusionMatrix(as.factor(predbin_ann), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 89.39%
#Kappa of 0.7723
```

## Decision Tree Model Creation
```{r, cache=TRUE}
library(C50)

droptree <- C5.0(as.factor(TargetGraduate) ~., data=droptrain)
```

## Decision Tree Model Testing
```{r, cache=TRUE}
tree_pred <- predict(droptree, droptest)
confusionMatrix(as.factor(tree_pred), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 90.36%
#Kappa of 0.7899
```

## Random Forest Model Creation
```{r, cache=TRUE}
library(randomForest)

forestmodel <- randomForest(as.factor(TargetGraduate) ~., data = droptrain)

varImpPlot(forestmodel)
```


## Random Forest Model Testing
```{r, cache=TRUE}
forest_pred <- predict(forestmodel, droptest)

confusionMatrix(as.factor(forest_pred), as.factor(droptest$TargetGraduate), positive = "1")

#Accuracy of 91.18%
#Kappa for this model is 0.8095
```

## Individual Initial Model Results
So far our most accurate model in predicting college dropouts has been our Random Forest model. It has the highest Kappa at 0.8095, and the highest accuracy at 91.18%. Though it isn't the most accurate model, we can also use our decision tree to find out which of our students' characteristics are the biggest predictors of dropping out.

Attribute Usage:

- 100.00%	Curricular.units.2nd.sem..approved.
- 72.73%	Tuition.fees.up.to.date1
- 58.47%	Curricular.units.2nd.sem..credited.
- 56.23%	Debtor1
- 32.99%	Course2




# Combining Models for a Stacked Decision Tree

## Making New Data Frames with Predictions
```{r}
# Adding Prediction Columns for Every Model
dropstack <- data.frame(droptest$TargetGraduate, Logpred, KNNmodel, ann_pred, tree_pred, forest_pred)

# Renaming Columns
colnames(dropstack)[1] = "Graduated"
colnames(dropstack)[3] = "KNNPred"
colnames(dropstack)[4] = "ANNPred"
colnames(dropstack)[5] = "TreePred"
colnames(dropstack)[6] = "ForestPred"

summary(dropstack)
```

## Test and Train for Stacked Data
```{r}
set.seed(12345)
trainrows <- sample(nrow(dropstack), 0.8*nrow(dropstack))

stack_train <- dropstack[trainrows, ]
stack_test <- dropstack[-trainrows, ]
```

## Decision Tree on Telestack
```{r}
library(C50)
stack_tree <- C5.0(as.factor(Graduated) ~., data = stack_train)

plot(stack_tree)
```

## Evaluating Decision Tree on Telestack
```{r}
stack_tree_pred <- predict(stack_tree, stack_test)

confusionMatrix(as.factor(stack_tree_pred), as.factor(stack_test$Graduated), positive = "1")
```


# Using our Model to Predict Enrolled Students
