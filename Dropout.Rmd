---
title: "Part 2: College Dropout Modeling and Prediction"
author: "The Kumar Kids"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: spacelab
    highlight: zenburn
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

### Background of the Data

In this report, we will be analyzing a dataset of students who have either graduated, dropped out, or are still enrolled from a four year undergraduate degree. This data, which is amalgamated from several different sources, includes information on demographic, socioeconomic, macroeconomic, and academic factors for these students. The success of students in higher education is vital to the overall productivity of an economy, as human capital is a huge driver of growth.

### Structure of the Data

This dataset has 35 overall columns, with 1 being our target variable and the others being characteristics of both the students and their environment. We have demographic information like marital status, gender, and age of the students. We also have financial information of the students like whether they are a debtor, have their tuition fees up to date, or are on a scholarship. There are also academic characteristics of these students such as their approved credits, course of study, or whether they have educational special needs. Lastly, there are macroeconomic indicators as well as they are strongly correlated with trends in education.

Our target variable for training the model will be whether or not the students graduated, and we will then use the models we built on the students that are currently enrolled to predict whether they have a high likelihood of dropping out. When we do our analysis, the column "TargetGraduate" indicates whether that student graduated with a value of "1" for graudate and "0" for non graduate.

### Use Case

We will be posing as a consulting group hired on by the board of directors of the Polytechnic Institute Of Portalegre. We were brought on in order to figure out why they have a much higher dropout rate than the average in the United States. Reports indicate that around 33% of undergraduates do not complete their degree program. Whereas in our data it appears that, among the combined students that have either dropped out or graduated, around 40% of the students did not graduate.

Knowing the characteristics of students who are likely to drop out, the academic advisors could further evaluate the situation of these “likely-to-dropout” students case by case, and reach out to offer academic planning/support to reduce their likelihood of dropping out. With the number of enrolled students who would likely drop out, the finance office can estimate the amount of tuition the university would lose per year with this number. The university admission office could also use this number to decide the number of transfer students we can allow next year to fill the gap. 


# Initial Model Creation

## Testing and Training Sets
```{r}
#Loading in CSV
drop_norm <- read.csv("cleaned_nonenrolls.csv")

#Setting Seed for Ease of Replication
set.seed(12345)
trainrows <- sample(nrow(drop_norm), 0.8*nrow(drop_norm))

#Training and Testing Sets with full columns
droptrain <- drop_norm[trainrows, ]
droptest <- drop_norm[-trainrows, ]

#Sets without Y variable
knn_train <- drop_norm[trainrows, -match("TargetGraduate", names(drop_norm))]
knn_test <- drop_norm[-trainrows, -match("TargetGraduate", names(drop_norm))]

#Sets with just Y variable
train_labels <- drop_norm[trainrows, "TargetGraduate"]
test_labels <- drop_norm[-trainrows, "TargetGraduate"]
```


## Building Logistic Model
```{r}
logmod <- glm(TargetGraduate ~., data = droptrain, family = "binomial")
```

## Testing Logistic Model
```{r, warning=FALSE}
library(caret)

Logpred <- predict(logmod, droptest)
pblog <- ifelse(Logpred >= 0.5, 1, 0)
cm <- confusionMatrix(as.factor(pblog), as.factor(droptest$Target), positive = "1")
cm

log_kappa <- cm$overall['Kappa']
log_accuracy <- cm$overall['Accuracy']
```

Logistic model performance

- Kappa: `r log_kappa`
- Accuracy: `r log_accuracy`


## KNN Model Creation
```{r, cache=TRUE}
library(class)

set.seed(12345)
KNNmodel <- knn(knn_train, knn_test, train_labels, k = 58)
```

## KNN Model Testing
```{r, cache=TRUE}
cm <- confusionMatrix(as.factor(KNNmodel), as.factor(test_labels), positive = "1")
cm

knn_kappa <- cm$overall['Kappa']
knn_accuracy <- cm$overall['Accuracy']
```

KNN model performance

- Kappa: `r knn_kappa`
- Accuracy: `r knn_accuracy`


## ANN Model Creation
We are building a Neural Network with 1 hidden layer.
```{r, cache= TRUE}
library(neuralnet)
model_ann <- neuralnet(TargetGraduate ~ ., data = droptrain, hidden = 1)
```

## ANN Model Testing
```{r, cache= TRUE}
library (caret)
ann_pred <- predict(model_ann, droptest)
predbin_ann <- ifelse(ann_pred >= 0.6, 1, 0)
cm <- confusionMatrix(as.factor(predbin_ann), as.factor(droptest$TargetGraduate), positive = "1")
cm

nn_accuracy <- cm$overall['Accuracy']
nn_kappa <- cm$overall['Kappa']
```

2-layer Neural Network model performance

- Kappa: `r nn_kappa`
- Accuracy: `r nn_accuracy`


## Decision Tree Model Creation
```{r, cache=TRUE}
library(C50)

droptree <- C5.0(as.factor(TargetGraduate) ~., data=droptrain)
```

## Decision Tree Model Testing
```{r, cache=TRUE}
tree_pred <- predict(droptree, droptest)
cm <- confusionMatrix(as.factor(tree_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

dt_accuracy <- cm$overall['Accuracy']
dt_kappa <- cm$overall['Kappa']
```

Decision Tree model performance

- Kappa: `r dt_kappa`
- Accuracy: `r dt_accuracy`

## Random Forest Model Creation
```{r, cache=TRUE}
library(randomForest)

forestmodel <- randomForest(as.factor(TargetGraduate) ~., data = droptrain)

varImpPlot(forestmodel)
```


## Random Forest Model Testing
```{r, cache=TRUE}
forest_pred <- predict(forestmodel, droptest)

cm <- confusionMatrix(as.factor(forest_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

rf_accuracy <- cm$overall['Accuracy']
rf_kappa <- cm$overall['Kappa']
```

Random Forest model performance

- Kappa: `r rf_kappa`
- Accuracy: `r rf_accuracy`


## Individual Initial Model Results
So far our most accurate model in predicting college dropouts has been our Random Forest model. It has the highest Kappa at `r sprintf("%0.2f", rf_kappa)`, and the highest accuracy at `r sprintf("%0.2f%%", rf_accuracy * 100)`. Though it isn't the most accurate model, we can also use our decision tree to find out which of our students' characteristics are the biggest predictors of dropping out.

Attribute Usage:

- 100.00%	Curricular.units.2nd.sem..approved.
- 72.73%	Tuition.fees.up.to.date1
- 58.47%	Curricular.units.2nd.sem..credited.
- 56.23%	Debtor1
- 32.99%	Course2




# Combining Models for a Stacked Decision Tree

## Making New Data Frames with Predictions
```{r}
# Adding Prediction Columns for Every Model
dropstack <- data.frame(droptest$TargetGraduate, Logpred, KNNmodel, ann_pred, tree_pred, forest_pred)

# Renaming Columns
colnames(dropstack)[1] = "Graduated"
colnames(dropstack)[3] = "KNNPred"
colnames(dropstack)[4] = "ANNPred"
colnames(dropstack)[5] = "TreePred"
colnames(dropstack)[6] = "ForestPred"

summary(dropstack)
```

## Test and Train for Stacked Data
```{r}
set.seed(12345)
trainrows <- sample(nrow(dropstack), 0.8*nrow(dropstack))

stack_train <- dropstack[trainrows, ]
stack_test <- dropstack[-trainrows, ]
```

## Decision Tree on Telestack
```{r}
library(C50)
stack_tree <- C5.0(as.factor(Graduated) ~., data = stack_train)

plot(stack_tree)
```

## Evaluating Decision Tree on Telestack
```{r}
stack_tree_pred <- predict(stack_tree, stack_test)

confusionMatrix(as.factor(stack_tree_pred), as.factor(stack_test$Graduated), positive = "1")
```


# Using our Models to Predict Enrolled Students

## Creating Predictions Using Our Models
```{r}
#Reading in our Enrolled Students
enrolled <- read.csv("cleaned_enrolls.csv")

#Predicting on our enrolled students using logistic model
log_enr_pred <- predict(logmod, enrolled)
pbenr <- ifelse(log_enr_pred >= 0.5, 1, 0)

#Predicting on our enrolled students using ANN model
ann_enr_pred <- predict(model_ann, enrolled)
pb_ann_enr <- ifelse(ann_enr_pred >= 0.6, 1, 0)

#Predicting on our enrolled students using decision tree
tree_enr_pred <- predict(droptree, enrolled)

#Predicting on our enrolled students using random forest
library(randomForest)
forest_enr_pred <- predict(forestmodel, enrolled)
```

## Adding predictions to dataframe
```{r}
enrolled_preds <- data.frame(pbenr, pb_ann_enr, tree_enr_pred, forest_enr_pred)
enrolled_preds$tree_enr_pred <- as.numeric(enrolled_preds$tree_enr_pred)
enrolled_preds$tree_enr_pred <- ifelse(enrolled_preds$tree_enr_pred == 2, 1, 0)
enrolled_preds$forest_enr_pred <- as.numeric(enrolled_preds$forest_enr_pred)
enrolled_preds$forest_enr_pred <- ifelse(enrolled_preds$forest_enr_pred == 2, 1, 0)


summary(enrolled_preds)
```

## Adding Prediction Column and Subsetting Enrolled Students
```{r}
enrolled$predictions <- pbenr

dropouts <- subset(enrolled, enrolled$predictions == "1")
graduates <- subset(enrolled, enrolled$predictions == "0")

dropmeans <- colMeans(dropouts)
gradmeans <- colMeans(graduates)

enrolledmeans <- data.frame(dropmeans, gradmeans)
```


## Analysis of our predictions on enrolled students

When we apply our models to the enrolled students, we can see that they all slightly overpredict the actual incidence of graduation.

Our models predict the following graduation rates:
- Logistic Model = 64%
- ANN Model = 72%
- Decision Tree = 71%
- Random Forest = 75%.

Whereas the actual rate of graduation is around 61%. The true utility comes from analyzing what of these predicted enrolled students have in common. For that we can add our predictions and compare the characteristics of those who were predicted to dropout vs. those who were predicted to graduate.
