---
title: "Part 2: College Dropout Modeling and Prediction"
author: "The Kumar Kids"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: spacelab
    highlight: zenburn
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

### Background of the Data

In this report, we will be analyzing a dataset of students who have either graduated, dropped out, or are still enrolled from a four year undergraduate degree. This data, which is amalgamated from several different sources, includes information on demographic, socioeconomic, macroeconomic, and academic factors for these students. The success of students in higher education is vital to the overall productivity of an economy, as human capital is a huge driver of growth.

### Structure of the Data

This dataset has 35 overall columns, with 1 being our target variable and the others being characteristics of both the students and their environment. We have demographic information like marital status, gender, and age of the students. We also have financial information of the students like whether they are a debtor, have their tuition fees up to date, or are on a scholarship. There are also academic characteristics of these students such as their approved credits, course of study, or whether they have educational special needs. Lastly, there are macroeconomic indicators as well as they are strongly correlated with trends in education.

Our target variable for training the model will be whether or not the students graduated, and we will then use the models we built on the students that are currently enrolled to predict whether they have a high likelihood of dropping out. When we do our analysis, the column "TargetGraduate" indicates whether that student graduated with a value of "1" for graudate and "0" for non graduate.

### Use Case

We will be posing as a consulting group hired on by the board of directors of the Polytechnic Institute Of Portalegre. We were brought on in order to figure out why they have a much higher dropout rate than the average in the United States. Reports indicate that around 33% of undergraduates do not complete their degree program. Whereas in our data it appears that, among the combined students that have either dropped out or graduated, around 40% of the students did not graduate.

Knowing the characteristics of students who are likely to drop out, the academic advisors could further evaluate the situation of these “likely-to-dropout” students case by case, and reach out to offer academic planning/support to reduce their likelihood of dropping out. With the number of enrolled students who would likely drop out, the finance office can estimate the amount of tuition the university would lose per year with this number. The university admission office could also use this number to decide the number of transfer students we can allow next year to fill the gap. 


# Initial Model Creation

In our analysis, we will be utilizing the steps laid out to us in TO 628 by Dr. Kumar. These steps include the loading, cleaning, and breaking into training and testing sets of our data. Followed by the creation of prediction models and there testing. After that, we see if we can iterate and improve! The models that we will be using to analyze our college students are:
- Logistic Regression
- K Nearest Neighbor
- Artificial Neural Network
- Decision Tree
- Random Forest Model

## Testing and Training Sets

```{r}
#Loading in CSV
drop_norm <- read.csv("cleaned_nonenrolls.csv")

#Setting Seed for Ease of Replication
set.seed(12345)
trainrows <- sample(nrow(drop_norm), 0.8*nrow(drop_norm))

#Training and Testing Sets with full columns
droptrain <- drop_norm[trainrows, ]
droptest <- drop_norm[-trainrows, ]

#Sets without Y variable
knn_train <- drop_norm[trainrows, -match("TargetGraduate", names(drop_norm))]
knn_test <- drop_norm[-trainrows, -match("TargetGraduate", names(drop_norm))]

#Sets with just Y variable
train_labels <- drop_norm[trainrows, "TargetGraduate"]
test_labels <- drop_norm[-trainrows, "TargetGraduate"]
```

The target variable `TargetGraduate` is a dummy variable with two values: 0 and 1. 

- 0: this student has dropped out from the university
- 1: this student successfully graduated from the university


## Building Logistic Model
```{r}
logmod <- glm(TargetGraduate ~., data = droptrain, family = "binomial")
```

## Testing Logistic Model
```{r, warning=FALSE}
library(caret)

Logpred <- predict(logmod, droptest)
pblog <- ifelse(Logpred >= 0.5, 1, 0)
cm <- confusionMatrix(as.factor(pblog), as.factor(droptest$TargetGraduate), positive = "1")
cm

log_kappa <- cm$overall['Kappa']
log_accuracy <- cm$overall['Accuracy']
```

Logistic model performance

- Kappa: `r log_kappa`
- Accuracy: `r log_accuracy`


## KNN Model Creation
```{r, cache=TRUE}
library(class)

set.seed(12345)
KNNmodel <- knn(knn_train, knn_test, train_labels, k = 58)
```

## KNN Model Testing
```{r, cache=TRUE}
cm <- confusionMatrix(as.factor(KNNmodel), as.factor(test_labels), positive = "1")
cm

knn_kappa <- cm$overall['Kappa']
knn_accuracy <- cm$overall['Accuracy']
```

KNN model performance

- Kappa: `r knn_kappa`
- Accuracy: `r knn_accuracy`


## ANN Model Creation
We are building a Neural Network with 1 hidden layer.
```{r, cache= TRUE}
library(neuralnet)
model_ann <- neuralnet(TargetGraduate ~ ., data = droptrain, hidden = 1)
```

## ANN Model Testing
```{r, cache= TRUE}
library (caret)
ann_pred <- predict(model_ann, droptest)
predbin_ann <- ifelse(ann_pred >= 0.5, 1, 0)
cm <- confusionMatrix(as.factor(predbin_ann), as.factor(droptest$TargetGraduate), positive = "1")
cm

nn_accuracy <- cm$overall['Accuracy']
nn_kappa <- cm$overall['Kappa']
```

2-layer Neural Network model performance

- Kappa: `r nn_kappa`
- Accuracy: `r nn_accuracy`


## Decision Tree Model Creation
```{r, cache=TRUE}
library(C50)

droptree <- C5.0(as.factor(TargetGraduate) ~., data=droptrain)
```

## Decision Tree Model Testing
```{r, cache=TRUE}
tree_pred <- predict(droptree, droptest)
cm <- confusionMatrix(as.factor(tree_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

dt_accuracy <- cm$overall['Accuracy']
dt_kappa <- cm$overall['Kappa']
```

Decision Tree model performance

- Kappa: `r dt_kappa`
- Accuracy: `r dt_accuracy`

## Random Forest Model Creation
```{r, cache=TRUE}
library(randomForest)

forestmodel <- randomForest(as.factor(TargetGraduate) ~., data = droptrain)
```


## Random Forest Model Testing
```{r, cache=TRUE}
forest_pred <- predict(forestmodel, droptest)

cm <- confusionMatrix(as.factor(forest_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

rf_accuracy <- cm$overall['Accuracy']
rf_kappa <- cm$overall['Kappa']
```

Random Forest model performance

- Kappa: `r rf_kappa`
- Accuracy: `r rf_accuracy`


## Individual Initial Model Results
So far our most accurate model in predicting college dropouts has been our Random Forest model. It has the highest Kappa at `r sprintf("%0.2f", rf_kappa)`, and the highest accuracy at `r sprintf("%0.2f%%", rf_accuracy * 100)`. What we can do next is combine the predictions in all of our models to create a second layer decision tree. The hope is that this second layer decision tree will capture all of the best parts of each of our models.


# Combining Models for a Stacked Decision Tree

## Making New Data Frames with Predictions
```{r}
# Adding Prediction Columns for Every Model
dropstack <- data.frame(droptest$TargetGraduate, Logpred, KNNmodel, ann_pred, tree_pred, forest_pred)

# Renaming Columns
colnames(dropstack)[1] = "Graduated"
colnames(dropstack)[3] = "KNNPred"
colnames(dropstack)[4] = "ANNPred"
colnames(dropstack)[5] = "TreePred"
colnames(dropstack)[6] = "ForestPred"

summary(dropstack)
```

## Test and Train for Stacked Data
```{r}
set.seed(12345)
trainrows <- sample(nrow(dropstack), 0.8*nrow(dropstack))

stack_train <- dropstack[trainrows, ]
stack_test <- dropstack[-trainrows, ]
```

## Decision Tree on Stacked Data
```{r}
library(C50)
stack_tree <- C5.0(as.factor(Graduated) ~., data = stack_train)

plot(stack_tree)
```

## Evaluating Decision Tree on Stacked Data
```{r}
stack_tree_pred <- predict(stack_tree, stack_test)

cm<- confusionMatrix(as.factor(stack_tree_pred), as.factor(stack_test$Graduated), positive = "1")
cm

stacked_kappa <- cm$overall['Kappa']
stacked_accuracy <- cm$overall['Accuracy']
```

Stacked Model Performance

- Kappa: `r stacked_kappa`
- Accuracy: `r stacked_accuracy`

From the testing of our decision tree model, we can see that it did indeed have a higher Kappa and Accuracy than *any* of our previous models! We can then create predictions for our enrolled students, and use this decision tree to make a final aggregated decision.

# Using our Models to Predict Enrolled Students

## Creating Predictions Using Our Models
```{r}
#Reading in our Enrolled Students
enrolled <- read.csv("cleaned_enrolls.csv")

#Predicting on our enrolled students using logistic model
log_enr_pred <- predict(logmod, enrolled)
pbenr <- ifelse(log_enr_pred >= 0.5, 1, 0)

#Predicting on our enrolled students using ANN model
ann_enr_pred <- predict(model_ann, enrolled)
pb_ann_enr <- ifelse(ann_enr_pred >= 0.5, 1, 0)

#Predicting on our enrolled students using decision tree
tree_enr_pred <- predict(droptree, enrolled)

#Predicting on our enrolled students using random forest
library(randomForest)
forest_enr_pred <- predict(forestmodel, enrolled)
```

## Adding predictions to dataframe
```{r}
enrolled_preds <- data.frame(pbenr, pb_ann_enr, tree_enr_pred, forest_enr_pred)
enrolled_preds$tree_enr_pred <- as.numeric(enrolled_preds$tree_enr_pred)
enrolled_preds$tree_enr_pred <- ifelse(enrolled_preds$tree_enr_pred == 2, 1, 0)
enrolled_preds$forest_enr_pred <- as.numeric(enrolled_preds$forest_enr_pred)
enrolled_preds$forest_enr_pred <- ifelse(enrolled_preds$forest_enr_pred == 2, 1, 0)


summary(enrolled_preds)
```

## Analysis of Our Current Predictions for Enrolled Students

When we apply our models to the enrolled students, we can see that they all slightly overpredict the actual incidence of graduation.

Our models predict the following graduation rates:
- Logistic Model = 64%
- ANN Model = 72%
- Decision Tree = 71%
- Random Forest = 75%.

Whereas the actual rate of graduation is around 61%. Since we will be missing some of the actual students who will be dropping out, let's see if we can improve some of our models abilities to catch those students.


# Building Improved Models

## Testing Improved Logistic Model

Improvements to our logistic regression model predictions actually don't even need to be made to the model itself. All we have to do is increase the prediction threshold for whether or not a student will graduate, and the model will predict less students overall graduating. Although we lose some of the accuracy, we have half as many false positive predictions (students that are predicted to graduate but do not). We go from 40 false graduates to 20 false graduates.

```{r}
#Increasing Threshold
pblog_impr <- ifelse(Logpred >= 1.6, 1, 0)
cm <- confusionMatrix(as.factor(pblog_impr), as.factor(droptest$Target), positive = "1")
cm 

log_impr_accuracy <- cm$overall['Accuracy']
log_impr_kappa <- cm$overall['Kappa']
```
Improved Logistic Model Performance: 

- Kappa: `r log_impr_accuracy`
- Accuracy: `r log_impr_kappa`


## Improved KNN Model Creation

For our KNN Model, what we are able to do to improve it is to lower our number of nearest neighbors. However, our model only gets to around 75% accuracy even with the lowest K. Not only that, but it is significantly overpredicting the number of students who will graduate, and missing many of the ones who will drop out. We conclude that KNN is not suitable for our predictions.

```{r, cache=TRUE}
library(class)

set.seed(12345)
KNNmodel2 <- knn(knn_train, knn_test, train_labels, k = 1)
```


## Improved KNN Model Testing
```{r, cache=TRUE}
confusionMatrix(as.factor(KNNmodel2), as.factor(test_labels), positive = "1")

#Accuracy of 75.07%
#Kappa of 0.4524
```


## Improved ANN Model Testing

Much like our Logistic Model, we do not actually have to create a new neural network in order to improve the ability of our model to catch false positives. All we have to do is increase the prediction threshold. Similar to the logistic regression model, we see our false positive decreases from around 41 to 34.

```{r, cache= TRUE}
library (caret)
predbin_ann_impr <- ifelse(ann_pred >= .9483, 1, 0)
cm <- confusionMatrix(as.factor(predbin_ann_impr), as.factor(droptest$TargetGraduate), positive = "1")
cm

ann_impr_accuracy <- cm$overall['Accuracy']
ann_impr_kappa <- cm$overall['Kappa']
```

Improved ANN Model Performance:

- Accuracy: `r ann_impr_accuracy`
- Kappa: `r ann_impr_kappa`


## Improved Decision Tree Model Creation

Unliked our Logistic Model or ANN model, we can create a new decision tree that takes into account a *cost matrix*. This matrix will tell the decision tree algorithm that one of our false cases is much more costly than the other. For this, we will assign a value of 3 to our false positives and 1 to our false negatives. With this type of error cost accounting, we can see a decrease in our false graduation predictions from around 50 to 26.

```{r, cache=TRUE}
library(C50)

#Creating our Error Cost Matrix
error_cost = matrix(c(0, 3, 1, 0), nrow = 2)
error_cost

#Creating our new decision tree factoring in cost matrix
errortree <- C5.0(as.factor(TargetGraduate) ~., data=droptrain, costs = error_cost)
```

## Improved Decision Tree Model Testing
```{r, cache=TRUE}
impr_tree_pred <- predict(errortree, droptest)
cm <- confusionMatrix(as.factor(impr_tree_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

dt_impr_accuracy <- cm$overall['Accuracy']
dt_impr_kappa <- cm$overall['Kappa']
```

Improved Decision Tree Model Performance:

- Accuracy: `r dt_impr_accuracy`
- Kappa: `r dt_impr_kappa`


## Improved Random Forest Model Creation

We can introduce a similar concept into our random forest model, but adding a "cutoff" parameter to the function. This will allow us to weight each of the target variable possibilities.

```{r, cache=TRUE}
library(randomForest)

errorforest <- randomForest(as.factor(TargetGraduate) ~., data = droptrain, cutoff = c(.35, .65))
```


## Improved Random Forest Model Testing
```{r, cache=TRUE}
error_forest_pred <- predict(errorforest, droptest)

cm <- confusionMatrix(as.factor(error_forest_pred), as.factor(droptest$TargetGraduate), positive = "1")
cm

rf_impr_accuracy <- cm$overall['Accuracy']
rf_impr_kappa <- cm$overall['Kappa']
```

Improved Random Forest Model Performance:

- Accuracy: `r rf_impr_accuracy`
- Kappa: `r rf_impr_kappa`


# Using our *Improved* Models to Predict Enrolled Students

## Creating Predictions Using Our Models
```{r}
#Reading in our Enrolled Students
new_enrolled <- read.csv("cleaned_enrolls.csv")

#Predicting on our enrolled students using logistic model
log_enr_pred <- predict(logmod, new_enrolled)
impr_pbenr <- ifelse(log_enr_pred >= 0.8, 1, 0)

#Predicting on our enrolled students using ANN model
ann_enr_pred <- predict(model_ann, new_enrolled)
impr_pb_ann_enr <- ifelse(ann_enr_pred >= 0.948348, 1, 0)

#Predicting on our enrolled students using decision tree
impr_tree_enr_pred <- predict(errortree, new_enrolled)

#Predicting on our enrolled students using random forest
impr_forest_enr_pred <- predict(errorforest, new_enrolled)
```

## Adding predictions to dataframe
```{r}
new_enrolled_preds <- data.frame(impr_pbenr, impr_pb_ann_enr, impr_tree_enr_pred, impr_forest_enr_pred)
new_enrolled_preds$impr_tree_enr_pred <- as.numeric(new_enrolled_preds$impr_tree_enr_pred)
new_enrolled_preds$impr_tree_enr_pred <- ifelse(new_enrolled_preds$impr_tree_enr_pred == 2, 1, 0)
new_enrolled_preds$impr_forest_enr_pred <- as.numeric(new_enrolled_preds$impr_forest_enr_pred)
new_enrolled_preds$impr_forest_enr_pred <- ifelse(new_enrolled_preds$impr_forest_enr_pred == 2, 1, 0)


summary(new_enrolled_preds)
```


## Adding Prediction Column and Subsetting Enrolled Students
```{r}
enrolled$predictions <- pbenr

dropouts <- subset(enrolled, enrolled$predictions == "1")
graduates <- subset(enrolled, enrolled$predictions == "0")

dropmeans <- colMeans(dropouts)
gradmeans <- colMeans(graduates)

enrolledmeans <- data.frame(dropmeans, gradmeans)
enrolledmeans
```



## Characteristics of Predicted Dropouts
So far our most accurate model in predicting college dropouts has been our Random Forest model. It has the highest Kappa at `r sprintf("%.4f", rf_impr_kappa)`, and the highest accuracy at `r sprintf("%.2f%%", rf_impr_accuracy)`. Though it isn't the most accurate model, we can also use our decision tree to find out which of our students' characteristics are the biggest predictors of dropping out.

Attribute Usage:

- 100.00%	Curricular.units.2nd.sem..approved.
- 72.73%	Tuition.fees.up.to.date1
- 58.47%	Curricular.units.2nd.sem..credited.
- 56.23%	Debtor1
- 32.99%	Course2

# Conclusion
We identify the number of curricular units approved in the second semester as well as timely payment of tuition fees to be the key drivers of the student dropout rate. These results are unsurprising that financial pressures such as tuition payment and pre-existing debt would cause students to ultimately dropout. Furthermore, the correlation between curricular units approved and credited in the second semester suggests student inability to properly craft feasible schedules significantly hampers their ability to graduate.

Firstly, we recommend addressing the financial issue through a number of means. Housing occupies a substantial portion of any student's budget, and providing affordable on or near-campus alternatives will assist in reducing this expense. Additionally, we also endorse a form of rolling financial in the form of scholarships or work-study opportunities that are available throughout students' expected four-year stay at the university. This will mitigate front loading financial aid to incoming students in their first year and allow for a more dynamic allocation of resources as needed. Each of these suggestions require a targeted approach that pinpoints those most in financial need. Hence, a vetting process of analyzing student income and wealth is necessary. Particularly, in the modern era, what composes "income" and "wealth" is constantly shifting; therefore, careful consideration of factors such as realized and unrealized income must be taken into consideration to reduce the risk of providing aid to those not realistically qualified as financially distressed.

Secondly, student curricular unit management concerns can be mitigated by softly introducing students to the collegiate curricular system prior to their arrival. This can take shape in a credit recognition program for high school students who have attended courses at local universities. In addition, we recommend a required freshman program or low-stakes course in which students are required to craft projected full four-year schedules that project their expected progress. As such, students will possess a framework to compare their expected and actual progress and adjust as necessary.

These recommendations provide a preliminary round of remedies to deeply systemic issues. Not only to we advocate implementing these measures, we endorse constant and consistent monitoring of their effects to more accurately target students with the highest probably to drop out yet be retained. This will allow for more effective resource allocation in optimizing student retention among and providing the admissions office with the necessary quantitative data to initiate the student transfer process. As such, this strategy furthers the Polytechnic Institute of Portalegre ability to achieve its goals of maximizing human capital, and providing its domestic economy with a more capable labor force.
